{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Wm1iDfR2XTt"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KDTree\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWZ5b7hn2XTw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ookV0k8Y2XTy"
      },
      "outputs": [],
      "source": [
        "def nearNeighborsSet(data, labels, k, input_x, y=None, n=0):\n",
        "    tree = KDTree(data)  # Construit un arbre KD à partir des points\n",
        "    dists, indices = tree.query([input_x], k=k+n)  # Trouve les k+n voisins les plus proches\n",
        "\n",
        "    neighbors = [(data[i], labels[i]) for i in indices[0]]  # Récupère les voisins avant suppression\n",
        "\n",
        "    if y is not None and n > 0:\n",
        "       # Keep all neighbors except those with label y\n",
        "        neighbors_without_y = [neighbor for neighbor in neighbors if neighbor[1] != y]\n",
        "        # If there are not enough neighbors without label y to fill k spots,\n",
        "        # take as many as possible, then fill up with neighbors with label y\n",
        "        if len(neighbors_without_y) < k:\n",
        "            neighbors_with_y = [neighbor for neighbor in neighbors if neighbor[1] == y][:k - len(neighbors_without_y)]\n",
        "            neighbors = neighbors_without_y + neighbors_with_y\n",
        "        else:\n",
        "            # If there are enough neighbors without label y, take the top k\n",
        "            neighbors = neighbors_without_y[:k]\n",
        "    else:\n",
        "        # If y is not specified, just return the k+n neighbors\n",
        "        neighbors = neighbors[:k+n]\n",
        "\n",
        "    return neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tnwTlVlQ2XTz"
      },
      "outputs": [],
      "source": [
        "def mostFreqLabel(data, labels, k, input_x, y=None, n=0):\n",
        "    neighbors = nearNeighborsSet(data, labels, k, input_x, y, n)\n",
        "    neighbor_labels = [label for _, label in neighbors]\n",
        "    return Counter(neighbor_labels).most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x_vPcFoy2XT0"
      },
      "outputs": [],
      "source": [
        "def QuickCertify(data, labels, n, input_x, K):\n",
        "    label_set = []\n",
        "    Kset = K  # Ensemble de valeurs de K à tester\n",
        "\n",
        "    for k in Kset:\n",
        "        # Obtenir l'étiquette la plus fréquente sans suppression\n",
        "        y_initial = mostFreqLabel(data, labels, k, input_x)\n",
        "        label_set.append(y_initial)\n",
        "        # Obtenir l'étiquette la plus fréquente avec suppression de n éléments de label y\n",
        "        y_after_removal = mostFreqLabel(data, labels, k, input_x, y_initial, n)\n",
        "\n",
        "        if y_initial != y_after_removal:\n",
        "            return False\n",
        "\n",
        "        if len(label_set) != 1:\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dOwW3na22XT1"
      },
      "outputs": [],
      "source": [
        "# # Charger l'ensemble de données Iris\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "# X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# # Exemple d'utilisation avec l'ensemble de données Iris\n",
        "# input_x = X[50]  # Utiliser un élément de l'ensemble Iris comme exemple\n",
        "# n = 5  # Nombre d'éléments à considérer comme potentiellement empoisonnés\n",
        "# label_y = y[50]  # Utiliser le label réel de l'input_x comme exemple de label à supprimer\n",
        "\n",
        "# # Appliquer QuickCertifyKD sur l'ensemble de données Iris\n",
        "# result = QuickCertify(X, y, n, input_x, label_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyF2AZfO2XT1"
      },
      "source": [
        "KNN LEARN INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hKLVq5vw2XT1"
      },
      "outputs": [],
      "source": [
        "def partition(T, p):\n",
        "    random.shuffle(T)\n",
        "    return [T[i::p] for i in range(p)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lx_BQIfz2XT1"
      },
      "outputs": [],
      "source": [
        "def KNNPredict(data, x, k):\n",
        "\n",
        "    # Convertir l'ensemble d'apprentissage en un tableau numpy pour les caractéristiques et les étiquettes\n",
        "    X_train = np.array([x for x, _ in data])\n",
        "    y_train = np.array([y for _, y in data])\n",
        "\n",
        "    # Créer un KDTree à partir de l'ensemble d'apprentissage\n",
        "    tree = KDTree(X_train)\n",
        "\n",
        "    # Trouver les indices des k voisins les plus proches\n",
        "    dists, indices = tree.query([x], k=k)\n",
        "\n",
        "    # Obtenir les étiquettes des k voisins les plus proches\n",
        "    k_nearest_labels = y_train[indices[0]]\n",
        "\n",
        "    # Retourner l'étiquette la plus commune parmi les k voisins\n",
        "    return Counter(k_nearest_labels).most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tNBDZZCP2XT2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def KNNLearnInit(T, p, K):\n",
        "    # Partitionner l'ensemble d'apprentissage T en p groupes (G1, G2, ..., Gp)\n",
        "    groups = partition(T, p)\n",
        "\n",
        "    candidate_values = K\n",
        "\n",
        "    errors = {Ki: [0 for _ in range(p)] for Ki in candidate_values}\n",
        "\n",
        "    errSet = [[[] for _ in range(p)] for _ in candidate_values]\n",
        "\n",
        "    for Ki in candidate_values:\n",
        "        # Calculer i la position de Ki dans candidate_values\n",
        "        i = candidate_values.index(Ki)\n",
        "\n",
        "        for j, Gj in enumerate(groups):\n",
        "            for x, y in Gj:\n",
        "                data = [x for x in T if not any(np.array_equal(x[0], element[0]) and x[1] == element[1] for element in Gj)]\n",
        "                if KNNPredict(data, x, Ki) != y:\n",
        "                    errSet[i][j].append((x,y))\n",
        "            errors[Ki][j] = len(errSet[i][j]) / len(Gj)\n",
        "\n",
        "\n",
        "        errors[Ki] = sum([errors[Ki][j] for j in range(p)]) / p\n",
        "\n",
        "    # Obtenir Ki avec l'erreur minimale\n",
        "    K_star = min(errors, key=errors.get)\n",
        "\n",
        "    Error_star = [(groups[j], errSet[candidate_values.index(K_star)][j]) for j in range(p)]\n",
        "\n",
        "    # for j in range(p):\n",
        "    #     print(\"G\" + str(j+1))\n",
        "    #     print(\"Ki: \", K_star)\n",
        "    #     print(\"Error: \", errors[K_star])\n",
        "    #     print(\"Gj: \", groups[j])\n",
        "    #     print(\"ErrSet: \", errSet[candidate_values.index(K_star)][j])\n",
        "\n",
        "\n",
        "    return K_star, Error_star\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1J6eQWO62XT2"
      },
      "outputs": [],
      "source": [
        "# X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# # # Créer une liste de paires (x, y) à partir de X et y\n",
        "# xy = list(zip(X, y))\n",
        "\n",
        "# k, error = KNNLearnInit(xy, 10)\n",
        "# print(k)\n",
        "# print(error[0][1])\n",
        "# print(error[1][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzdx8Jua2XT3"
      },
      "source": [
        "GEN PROMISING SUBSETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k4phZF3z2XT3"
      },
      "outputs": [],
      "source": [
        "def generateSubsetsR1(data, k, input_x, n, min_rmv):\n",
        "    \"\"\"\n",
        "    Génère des sous-ensembles R1 à partir des k+n éléments les plus proches de x.\n",
        "    \"\"\"\n",
        "    features = [point[0] for point in data]\n",
        "    labels = [point[1] for point in data]\n",
        "    nearest_neighbors = nearNeighborsSet(features, labels, k, input_x, n=n)\n",
        "\n",
        "    subsets_r1 = [list(subset) for r in range(min_rmv, len(nearest_neighbors) + 1)\n",
        "                                for subset in combinations(nearest_neighbors, r)]\n",
        "    return subsets_r1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C-IwpEmo2XT4"
      },
      "outputs": [],
      "source": [
        "def generateSubsetsR2(data, k, input_x, n, len_r1):\n",
        "    \"\"\"\n",
        "    Supprime d'abord tous les k+n éléments les plus proches de x dans data, puis crée tous les sous-ensembles\n",
        "    d'au plus n-len(r1) éléments avec le nouvel ensemble de données.\n",
        "    \"\"\"\n",
        "    features = [point[0] for point in data]\n",
        "    labels = [point[1] for point in data]\n",
        "    nearest_neighbors = nearNeighborsSet(features, labels, k, input_x, n=n)\n",
        "\n",
        "    # Utiliser la conversion en tuple pour la comparaison et éviter l'erreur\n",
        "    reduced_data =  [x for x in data if not any(np.array_equal(x, element) for element in nearest_neighbors)]\n",
        "\n",
        "    max_subset_size = n - len_r1\n",
        "    subsets_r2 = []\n",
        "    for r in range(1, max_subset_size + 1):\n",
        "        for subset in combinations(reduced_data, r):\n",
        "            subsets_r2.append(subset)\n",
        "    return subsets_r2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YN-mZzac2XT4"
      },
      "outputs": [],
      "source": [
        "def GenPromisingSubset(data, input_x, n, y, K):\n",
        "    \"\"\"\n",
        "    Utilise une recherche binaire pour trouver min_rmv, puis génère des sous-ensembles R1 et R2.\n",
        "    \"\"\"\n",
        "    promising_subsets = []\n",
        "    Kset = K  # Ensemble des valeurs de K à considérer\n",
        "\n",
        "    features = [point[0] for point in data]\n",
        "    labels = [point[1] for point in data]\n",
        "\n",
        "    for k in Kset:\n",
        "\n",
        "        # Initialiser les variables pour la recherche binaire\n",
        "        start, end = 1, n  # On commence à 1 car 0 ne changerait pas l'ensemble des voisins\n",
        "\n",
        "        # Recherche binaire pour trouver le min_rmv optimal\n",
        "        while start <= end:\n",
        "            mid = (start + end) // 2\n",
        "            if y == mostFreqLabel(features, labels, k, input_x, y, mid):\n",
        "                start = mid + 1\n",
        "            else:\n",
        "                end = mid - 1\n",
        "\n",
        "        min_rmv = start  # Le nombre minimal de suppressions pour changer la prédiction\n",
        "\n",
        "        # Générer des sous-ensembles R1 avec au moins min_rmv éléments\n",
        "        subsets_r1 = generateSubsetsR1(data, k, input_x, n, min_rmv)\n",
        "\n",
        "        for r1 in subsets_r1:\n",
        "            # Générer des sous-ensembles R2 avec le reste des données, jusqu'à n-len(r1) éléments\n",
        "            subsets_r2 = generateSubsetsR2(data, k, input_x, n, len(r1))\n",
        "\n",
        "            for r2 in subsets_r2:\n",
        "                # Combinaison de R1 et R2 pour former un sous-ensemble prometteur\n",
        "                R = r1 + list(r2)\n",
        "                # Retirer les éléments de R de l'ensemble de données original pour obtenir le sous-ensemble final\n",
        "                T_without_R = [x for x in data if not any(np.array_equal(x, element) for element in R)]\n",
        "\n",
        "                promising_subsets.append(T_without_R)\n",
        "\n",
        "    return promising_subsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HfzJQ5To2XT4"
      },
      "outputs": [],
      "source": [
        "# from sklearn.datasets import load_iris\n",
        "# X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# # Créer une liste de paires (x, y) à partir de X et y\n",
        "\n",
        "# xy = list(zip(X, y))\n",
        "# # print((xy))\n",
        "\n",
        "# # Afficher les 5 premières paires\n",
        "# print(len(GenPromisingSubset(data=xy[:149], input_x=[4.9, 3. , 1.4, 0.2], n=3, y='0', K=[3, 5, 7])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQRg0hSl2XT5"
      },
      "source": [
        "KNN LEARN UPDATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L2wkL5FP2XT5"
      },
      "outputs": [],
      "source": [
        "def compute_inflSet(R, groups):\n",
        "    \"\"\"\n",
        "    Calcule l'ensemble des points influencés par R.\n",
        "\n",
        "    :param R: Ensemble des points à supprimer.\n",
        "    :param groups: Les groupes actuels de points de données.\n",
        "    :return: Un ensemble de points influencés par R.\n",
        "    \"\"\"\n",
        "    # Pour simplifier, disons que tous les points sont influencés par R.\n",
        "    # Dans une implémentation réelle, vous devrez vérifier si la suppression\n",
        "    # de points dans R affecte les voisins les plus proches des points dans groups.\n",
        "\n",
        "    result = [x for x in groups if not any(np.array_equal(x[0], element[0]) and x[1] == element[1] for element in R)]\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gWsoQ42N2XT5"
      },
      "outputs": [],
      "source": [
        "def ensemble(errSetGj_Ki, R, newSet_minus, newSet_plus):\n",
        "\n",
        "    # Retirer les indices R et newSet_minus de errSetGj_Ki\n",
        "    result = np.setdiff1d(errSetGj_Ki, R)\n",
        "    result = np.setdiff1d(errSetGj_Ki, newSet_minus)\n",
        "\n",
        "    # Ajouter les indices de newSet_plus à errSetGj_Ki\n",
        "    result = np.union1d(errSetGj_Ki, newSet_plus)\n",
        "\n",
        "    # Le tableau errSetGj_Ki est maintenant mis à jour\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "skMcNA2B2XT6"
      },
      "outputs": [],
      "source": [
        "def KNNLearnUpdate(T, R, Error, candidate_k_values):\n",
        "    \"\"\"\n",
        "    Mise à jour incrémentielle du modèle KNN.\n",
        "\n",
        "    :param R: Ensemble des éléments à supprimer de l'entraînement.\n",
        "    :param Error: Contient des groupes et des ensembles d'erreurs.\n",
        "    :param candidate_k_values: Les valeurs k candidates pour le KNN.\n",
        "    :return: Le meilleur k basé sur la plus petite erreur.\n",
        "    \"\"\"\n",
        "    # Récupération des groupes et des ensembles d'erreurs\n",
        "    groups = []\n",
        "    err_sets = []\n",
        "    for elt in Error:\n",
        "        groups.append(elt[0])\n",
        "        err_sets.append(elt[1])\n",
        "\n",
        "    # Calcul des nouveaux groupes en supprimant R\n",
        "    new_groups = []\n",
        "    for elt in groups:\n",
        "        ensemble = [x for x in elt if not any(np.array_equal(x[0], element[0]) and x[1] == element[1] for element in R)]\n",
        "        new_groups.append(ensemble)\n",
        "\n",
        "    # Création du nouvel ensemble d'entraînement T'\n",
        "    T_prime = []\n",
        "    for elt in new_groups:\n",
        "        for i in elt:\n",
        "            T_prime.append(i)\n",
        "\n",
        "    # Initialisation de l'ensemble influencé par R\n",
        "    inflSet = compute_inflSet(R, groups)  # Cette fonction doit être définie\n",
        "\n",
        "    # Initialisation du dictionnaire pour les erreurs de chaque valeur k\n",
        "    error_prime = []\n",
        "\n",
        "    err_sets_prime = []\n",
        "\n",
        "    errors_for_k = []\n",
        "\n",
        "    # Itération sur chaque valeur k candidate\n",
        "    for k in candidate_k_values:\n",
        "        i = candidate_k_values.index(k)\n",
        "        # Itération sur chaque groupe\n",
        "        for j, Gj_prime in enumerate(new_groups):\n",
        "            newSet_minus = []\n",
        "            newSet_plus = []\n",
        "            # Itération sur chaque élément de données dans le groupe influencé\n",
        "            interGj_prime_inflSet = [x for x in Gj_prime if any(np.array_equal(x[0], element[0]) and x[1] == element[1] for element in inflSet)]\n",
        "            for x, y in interGj_prime_inflSet:\n",
        "                # Prédiction sur l'ensemble d'entraînement original\n",
        "                data = [x for x in T if not any(np.array_equal(x[0], element[0]) and x[1] == element[1] for element in groups[j])]\n",
        "                data_prime = [x for x in T_prime if not any(np.array_equal(x[0], element[0]) and x[1] == element[1] for element in Gj_prime)]\n",
        "                if KNNPredict(data, x, k) == y and KNNPredict(data_prime, x, k) != y:\n",
        "                    newSet_plus.add((x, y))\n",
        "\n",
        "                if KNNPredict(data, x, k) != y and KNNPredict(data_prime, x, k) == y:\n",
        "                    newSet_minus.add((x, y))\n",
        "\n",
        "            # Mise à jour de l'ensemble des erreurs pour le groupe j\n",
        "            err_sets_prime[i][j] = ensemble(err_sets[j], R, newSet_minus, newSet_plus)\n",
        "            error_prime[i][j] = len(err_sets[Gj_prime]) / len(Gj_prime)\n",
        "\n",
        "        # Calcul de l'erreur moyenne pour la valeur k\n",
        "        avg_error_k = sum([error_prime[i][j] for j in range(len(groups))]) / len(groups)\n",
        "        errors_for_k[i] = avg_error_k\n",
        "\n",
        "    # Trouver le meilleur k avec la plus petite erreur moyenne\n",
        "    best_k = min(errors_for_k, key=errors_for_k.get)\n",
        "\n",
        "    return best_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyhshopi2XT6"
      },
      "source": [
        "MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5nYG3ZNC2XT6"
      },
      "outputs": [],
      "source": [
        "def main(data, n, x):\n",
        "\n",
        "    X = [point[0] for point in data]\n",
        "    y = [point[1] for point in data]\n",
        "\n",
        "\n",
        "    Kcandidates = [1, 2 , 4, 6, 8]\n",
        "\n",
        "    if QuickCertify(X, y, n, x, Kcandidates):\n",
        "        return 'Certified'\n",
        "\n",
        "    K, Error = KNNLearnInit(data, 10, Kcandidates)\n",
        "\n",
        "    y = KNNPredict(data, x, K)\n",
        "\n",
        "    promised_set = GenPromisingSubset(data, x, n, y, Kcandidates)\n",
        "\n",
        "    while len(promised_set) > 0:\n",
        "        set_prime = promised_set[0]\n",
        "        promised_set.pop(0)\n",
        "\n",
        "        R = [x for x in data if not any([np.array_equal(x[0], y[0]) for y in set_prime])]\n",
        "        K_prime = KNNLearnUpdate(data, R, Error, Kcandidates)\n",
        "        y_prime = KNNPredict(set_prime, x, K_prime)\n",
        "\n",
        "        if y != y_prime:\n",
        "            return 'Falsified', R\n",
        "\n",
        "    if len(promised_set) == 0:\n",
        "        return 'Certified'\n",
        "    else:\n",
        "        return 'Unknown'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-87FAXm32XT7"
      },
      "source": [
        "TESTTT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "_E3vnznL2XT7",
        "outputId": "7c43bee0-3ed4-4e71-f379-59f948e7d58a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Create a list of pairs (x, y) from X and y\n",
        "data = list(zip(X, y))\n",
        "\n",
        "# Initialize counters\n",
        "falsification = 0\n",
        "certificat = 0\n",
        "inconnu = 0\n",
        "\n",
        "# Set the poisoning threshold\n",
        "seuil_poisoning = 3\n",
        "\n",
        "# Select 15 random data points\n",
        "i = 0\n",
        "data_input = []\n",
        "while i < 15:\n",
        "    index = np.random.randint(0, len(data))\n",
        "    data_input.append(data[index])\n",
        "\n",
        "    # Remove the selected data point from the list\n",
        "    data.pop(index)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "# Classify each data point\n",
        "for element in data_input:\n",
        "    print(element)\n",
        "    result = main(data, seuil_poisoning, element[0])\n",
        "    if result == 'Falsified':\n",
        "        falsification += 1\n",
        "    elif result == 'Certified':\n",
        "        certificat += 1\n",
        "    else:\n",
        "        inconnu += 1\n",
        "\n",
        "# Print the results\n",
        "print('Falsified:', falsification)\n",
        "print('Certified:', certificat)\n",
        "print('Unknown:', inconnu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
